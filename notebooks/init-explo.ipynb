{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:29:18.812319Z",
     "start_time": "2023-12-07T14:29:18.786139Z"
    }
   },
   "outputs": [],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "import requests\n",
    "import nltk\n",
    "from fake_useragent import UserAgent\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "keywords = ['xrp',\n",
    "            'XRP',\n",
    "            'Ripple Price Prediction',\n",
    "            'XRP prediction']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:44:57.532793Z",
     "start_time": "2023-12-07T14:44:57.526197Z"
    }
   },
   "id": "90c6a2af66a8cac8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nicolasasmann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "user_agent = UserAgent() \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:44:57.865202Z",
     "start_time": "2023-12-07T14:44:57.857207Z"
    }
   },
   "id": "d3259f96f713c48d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_news(period, path):\n",
    "    headers = {'User-Agent': user_agent.random, \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "    googlenews=GoogleNews(lang=\"en\")\n",
    "    googlenews.set_period(period)\n",
    "    print(\"Scraping started.\")\n",
    "    index = 0\n",
    "    scraped = []\n",
    "    data_to_save = []\n",
    "    titles = []\n",
    "    for key in keywords:\n",
    "        googlenews.get_news(key=key)\n",
    "        articles = googlenews.result()\n",
    "        for article in articles:\n",
    "            title = article[\"title\"]\n",
    "            if title in titles:\n",
    "                continue\n",
    "            titles.append(title)\n",
    "            url = article[\"link\"]\n",
    "            published_date = article[\"datetime\"]\n",
    "            try:\n",
    "                r = requests.get(f\"https://{url}\", timeout=15, headers=headers) \n",
    "                url = r.url\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not url in scraped:\n",
    "                news_article = Article(url=url, language=\"en\")\n",
    "                try:\n",
    "                    news_article.download()\n",
    "                    news_article.parse()\n",
    "                    news_article.nlp()\n",
    "                    for c_key in keywords:\n",
    "                        if c_key in news_article.text:\n",
    "                            text = news_article.text.replace(\"\\n\\n\", \"\\n\")\n",
    "                            data_to_save.append(f\"{published_date}\\n{news_article.title}\\n{text}\")\n",
    "                            index += 1\n",
    "                            print(f\"Sources scraped: {index}\")\n",
    "                            break\n",
    "                    scraped.append(url)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    data_to_save=list(set(data_to_save))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\\n\".join(data_to_save))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:44:58.230538Z",
     "start_time": "2023-12-07T14:44:58.220089Z"
    }
   },
   "id": "1b10afae3e6b6359"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping started.\n",
      "Sources scraped: 1\n",
      "Sources scraped: 2\n",
      "Sources scraped: 3\n",
      "Sources scraped: 4\n",
      "Sources scraped: 5\n",
      "Sources scraped: 6\n",
      "Sources scraped: 7\n",
      "Sources scraped: 8\n",
      "Sources scraped: 9\n",
      "Sources scraped: 10\n",
      "Sources scraped: 11\n",
      "Sources scraped: 12\n",
      "Sources scraped: 13\n",
      "Sources scraped: 14\n",
      "Sources scraped: 15\n",
      "Sources scraped: 16\n",
      "Sources scraped: 17\n",
      "Sources scraped: 18\n",
      "Sources scraped: 19\n",
      "Sources scraped: 20\n",
      "Sources scraped: 21\n",
      "Sources scraped: 22\n",
      "Sources scraped: 23\n",
      "Sources scraped: 24\n",
      "Sources scraped: 25\n",
      "Sources scraped: 26\n",
      "Sources scraped: 27\n",
      "Sources scraped: 28\n",
      "Sources scraped: 29\n",
      "Sources scraped: 30\n",
      "Sources scraped: 31\n",
      "Sources scraped: 32\n",
      "Sources scraped: 33\n",
      "Sources scraped: 34\n",
      "Sources scraped: 35\n",
      "Sources scraped: 36\n",
      "Sources scraped: 37\n",
      "Sources scraped: 38\n",
      "Sources scraped: 39\n",
      "Sources scraped: 40\n",
      "Sources scraped: 41\n",
      "Sources scraped: 42\n",
      "Sources scraped: 43\n",
      "Sources scraped: 44\n",
      "Sources scraped: 45\n",
      "Sources scraped: 46\n",
      "Sources scraped: 47\n",
      "Sources scraped: 48\n",
      "Sources scraped: 49\n",
      "Sources scraped: 50\n",
      "Sources scraped: 51\n",
      "Sources scraped: 52\n",
      "Sources scraped: 53\n",
      "Sources scraped: 54\n",
      "Sources scraped: 55\n",
      "Sources scraped: 56\n",
      "Sources scraped: 57\n",
      "Sources scraped: 58\n",
      "Sources scraped: 59\n",
      "Sources scraped: 60\n",
      "Sources scraped: 61\n",
      "Sources scraped: 62\n",
      "Sources scraped: 63\n",
      "Sources scraped: 64\n",
      "Sources scraped: 65\n",
      "Sources scraped: 66\n",
      "Sources scraped: 67\n",
      "Sources scraped: 68\n",
      "Sources scraped: 69\n",
      "Sources scraped: 70\n",
      "Sources scraped: 71\n",
      "Sources scraped: 72\n",
      "Data saved to data\n"
     ]
    }
   ],
   "source": [
    "path_txt = input(\"Path to save data to [.txt]: \")\n",
    "date = input(\"Period of news, e.g. [2h] = 2hours or [3d] = 3days: \")\n",
    "get_news(period=date, path=path_txt)\n",
    "print(f\"Data saved to {path_txt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T14:46:59.485913Z",
     "start_time": "2023-12-07T14:44:58.783284Z"
    }
   },
   "id": "61a71e42c78c0e47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4bdf34f11fbe2223"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
