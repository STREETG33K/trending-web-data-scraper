{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:19:55.513542Z",
     "start_time": "2023-12-07T15:19:40.891565Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "import requests\n",
    "import nltk\n",
    "from fake_useragent import UserAgent\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "sia = TextClassifier.load('en-sentiment')\n",
    "\n",
    "def smart_tokenize_and_preprocess(text):\n",
    "    words = word_tokenize(text)\n",
    "    result = [stemmer.stem(token.lower()) for token in words]\n",
    "    return result\n",
    "\n",
    "def flair_prediction(x):\n",
    "    # x = smart_tokenize_and_preprocess(x)\n",
    "    sentence = Sentence(x)\n",
    "    sia.predict(sentence)\n",
    "    score = sentence.labels[0]\n",
    "    if \"POSITIVE\" in str(score):\n",
    "        return \"POSITIVE\"\n",
    "    elif \"NEGATIVE\" in str(score):\n",
    "        return \"NEGATIVE\"\n",
    "    else:\n",
    "        return \"NEUTRAL\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:23:09.951652Z",
     "start_time": "2023-12-07T15:23:07.488268Z"
    }
   },
   "id": "187eb5d790d6663d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "keywords = ['xrp',\n",
    "            'XRP',\n",
    "            'Ripple Price Prediction',\n",
    "            'XRP prediction']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:19:55.521579Z",
     "start_time": "2023-12-07T15:19:55.520844Z"
    }
   },
   "id": "22b66d5d53073965"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nicolasasmann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "user_agent = UserAgent() \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:19:55.665680Z",
     "start_time": "2023-12-07T15:19:55.522321Z"
    }
   },
   "id": "46dc7bcaa55686ce"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "\n",
    "def get_news(period, path):\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    headers = {'User-Agent': user_agent.random, \"Upgrade-Insecure-Requests\": \"1\",\"DNT\": \"1\",\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\"Accept-Language\": \"en-US,en;q=0.5\",\"Accept-Encoding\": \"gzip, deflate\"}\n",
    "    googlenews=GoogleNews(lang=\"en\")\n",
    "    googlenews.set_period(period)\n",
    "    print(\"Scraping started.\")\n",
    "    index = 0\n",
    "    scraped = []\n",
    "    data_to_save = []\n",
    "    titles = []\n",
    "    for key in keywords:\n",
    "        googlenews.get_news(key=key)\n",
    "        articles = googlenews.result()\n",
    "        for article in articles:\n",
    "            title = article[\"title\"]\n",
    "            if title in titles:\n",
    "                continue\n",
    "            titles.append(title)\n",
    "            url = article[\"link\"]\n",
    "            published_date = article[\"datetime\"]\n",
    "            try:\n",
    "                r = requests.get(f\"https://{url}\", timeout=15, headers=headers) \n",
    "                url = r.url\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not url in scraped:\n",
    "                news_article = Article(url=url, language=\"en\")\n",
    "                try:\n",
    "                    news_article.download()\n",
    "                    news_article.parse()\n",
    "                    news_article.nlp()\n",
    "                    for c_key in keywords:\n",
    "                        if c_key in news_article.text:\n",
    "                            text = news_article.text.replace(\"\\n\\n\", \"\\n\")\n",
    "                            data_to_save.append(f\"{published_date}\\n{news_article.title}\\n{text}\")\n",
    "                            \n",
    "                            verdict = flair_prediction(text)\n",
    "                            if verdict == \"POSITIVE\": score += 1\n",
    "                            elif verdict == \"NEGATIVE\": score -= 1\n",
    "                            print(verdict)\n",
    "                            \n",
    "                            index += 1\n",
    "                            print(f\"Sources scraped: {index}\")\n",
    "                            break\n",
    "                    scraped.append(url)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    data_to_save=list(set(data_to_save))\n",
    "    \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\\n\".join(data_to_save))\n",
    "    score = score/index\n",
    "    print(\"RESULTS\")\n",
    "    print(f\"Score Results: {score}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:28:13.128174Z",
     "start_time": "2023-12-07T15:28:13.110139Z"
    }
   },
   "id": "23b7b5f69d5616db"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping started.\n",
      "NEGATIVE\n",
      "Sources scraped: 1\n",
      "NEGATIVE\n",
      "Sources scraped: 2\n",
      "POSITIVE\n",
      "Sources scraped: 3\n",
      "NEGATIVE\n",
      "Sources scraped: 4\n",
      "NEGATIVE\n",
      "Sources scraped: 5\n",
      "NEGATIVE\n",
      "Sources scraped: 6\n",
      "NEGATIVE\n",
      "Sources scraped: 7\n",
      "NEGATIVE\n",
      "Sources scraped: 8\n",
      "NEGATIVE\n",
      "Sources scraped: 9\n",
      "NEGATIVE\n",
      "Sources scraped: 10\n",
      "NEGATIVE\n",
      "Sources scraped: 11\n",
      "NEGATIVE\n",
      "Sources scraped: 12\n",
      "POSITIVE\n",
      "Sources scraped: 13\n",
      "NEGATIVE\n",
      "Sources scraped: 14\n",
      "NEGATIVE\n",
      "Sources scraped: 15\n",
      "NEGATIVE\n",
      "Sources scraped: 16\n",
      "POSITIVE\n",
      "Sources scraped: 17\n",
      "NEGATIVE\n",
      "Sources scraped: 18\n",
      "POSITIVE\n",
      "Sources scraped: 19\n",
      "POSITIVE\n",
      "Sources scraped: 20\n",
      "NEGATIVE\n",
      "Sources scraped: 21\n",
      "NEGATIVE\n",
      "Sources scraped: 22\n",
      "POSITIVE\n",
      "Sources scraped: 23\n",
      "NEGATIVE\n",
      "Sources scraped: 24\n",
      "NEGATIVE\n",
      "Sources scraped: 25\n",
      "NEGATIVE\n",
      "Sources scraped: 26\n",
      "NEGATIVE\n",
      "Sources scraped: 27\n",
      "POSITIVE\n",
      "Sources scraped: 28\n",
      "NEGATIVE\n",
      "Sources scraped: 29\n",
      "NEGATIVE\n",
      "Sources scraped: 30\n",
      "POSITIVE\n",
      "Sources scraped: 31\n",
      "NEGATIVE\n",
      "Sources scraped: 32\n",
      "POSITIVE\n",
      "Sources scraped: 33\n",
      "POSITIVE\n",
      "Sources scraped: 34\n",
      "NEGATIVE\n",
      "Sources scraped: 35\n",
      "POSITIVE\n",
      "Sources scraped: 36\n",
      "POSITIVE\n",
      "Sources scraped: 37\n",
      "POSITIVE\n",
      "Sources scraped: 38\n",
      "POSITIVE\n",
      "Sources scraped: 39\n",
      "NEGATIVE\n",
      "Sources scraped: 40\n",
      "NEGATIVE\n",
      "Sources scraped: 41\n",
      "NEGATIVE\n",
      "Sources scraped: 42\n",
      "NEGATIVE\n",
      "Sources scraped: 43\n",
      "NEGATIVE\n",
      "Sources scraped: 44\n",
      "POSITIVE\n",
      "Sources scraped: 45\n",
      "NEGATIVE\n",
      "Sources scraped: 46\n",
      "NEGATIVE\n",
      "Sources scraped: 47\n",
      "POSITIVE\n",
      "Sources scraped: 48\n",
      "NEGATIVE\n",
      "Sources scraped: 49\n",
      "NEGATIVE\n",
      "Sources scraped: 50\n",
      "NEGATIVE\n",
      "Sources scraped: 51\n",
      "NEGATIVE\n",
      "Sources scraped: 52\n",
      "NEGATIVE\n",
      "Sources scraped: 53\n",
      "NEGATIVE\n",
      "Sources scraped: 54\n",
      "POSITIVE\n",
      "Sources scraped: 55\n",
      "POSITIVE\n",
      "Sources scraped: 56\n",
      "NEGATIVE\n",
      "Sources scraped: 57\n",
      "NEGATIVE\n",
      "Sources scraped: 58\n",
      "POSITIVE\n",
      "Sources scraped: 59\n",
      "POSITIVE\n",
      "Sources scraped: 60\n",
      "NEGATIVE\n",
      "Sources scraped: 61\n",
      "NEGATIVE\n",
      "Sources scraped: 62\n",
      "NEGATIVE\n",
      "Sources scraped: 63\n",
      "NEGATIVE\n",
      "Sources scraped: 64\n",
      "NEGATIVE\n",
      "Sources scraped: 65\n",
      "POSITIVE\n",
      "Sources scraped: 66\n",
      "NEGATIVE\n",
      "Sources scraped: 67\n",
      "NEGATIVE\n",
      "Sources scraped: 68\n",
      "POSITIVE\n",
      "Sources scraped: 69\n",
      "NEGATIVE\n",
      "Sources scraped: 70\n",
      "NEGATIVE\n",
      "Sources scraped: 71\n",
      "POSITIVE\n",
      "Sources scraped: 72\n",
      "NEGATIVE\n",
      "Sources scraped: 73\n",
      "POSITIVE\n",
      "Sources scraped: 74\n",
      "RESULTS\n",
      "Score Results: -0.35135135135135137\n",
      "Data saved to data\n"
     ]
    }
   ],
   "source": [
    "path_txt = input(\"Path to save data to [.txt]: \")\n",
    "date = input(\"Period of news, e.g. [2h] = 2hours or [3d] = 3days: \")\n",
    "get_news(period=date, path=path_txt)\n",
    "print(f\"Data saved to {path_txt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T15:31:43.501356Z",
     "start_time": "2023-12-07T15:28:48.784509Z"
    }
   },
   "id": "fa4506984610843f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2283a77ceee0b540"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
